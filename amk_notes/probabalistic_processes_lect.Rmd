---
title: "Probabilistic processes"
author: "Tad Dallas"
includes:
  in_header:
    - \usepackage{lmodern}
output:
  pdf_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    toc: yes
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    highlight: tango
    theme: journal
---





## Logistic model revisited 



```{r}

logisticGrowth <- function(n, r, k){
  n*exp(r*(1-(n / k)))
}


logisticDynamics <- function(n,r,k, steps=100){
  ret <- c()
  ret[1] <- n
  if(length(r) == 1){
    r <- rep(r, steps)
  }
  for(i in 1:(steps-1)){
    ret[i+1] <- logisticGrowth(ret[i], r[i], k)
  }
  return(ret)
}
```


But this is not how populations change over time, right? 



Why not? 

+ No individual variation in birth rates
+ No temporal changes to carrying capacity or growth rate

but perhaps most importantly ... 




### Birth and death are probabilistic processes

This process is often called 'demographic stochasticity', and is especially important when thinking about small population sizes. As a thought experiment, imagine flipping a fair coin 5 times. The probability of landing on 'heads' is 0.5, but with only 5 trials, the resulting number of heads is far more variable than if we flipped that same coin 100 times. Considering birth and death as probabilistic processes, we can start to understand how we might expect population dynamics to be more variable at small population sizes. That is, the effects of 'demographic stochasticity' are dependent on population density. 










What if we treated birth as offspring drawn from a Poisson distribution, with some mean number of offspring $\lambda$? 

```{r}

logisticP1 <- function(Nt, b=0.1, d=0.1) {
  births <- sum(rbinom(Nt,1,b))
  deaths <- 0
  pop <- (Nt + births - deaths) 
  return(pop)
}

```

So the model above treats birth as drawn from a Poisson distribution, and death as dependent on the population density.




What if we treated death as binomial, with some probability $p$? 


```{r}

logisticP2 <- function(Nt, b=0.1, d=0.1) {
  births <- sum(rbinom(Nt, 1, b))
  deaths <- sum(rbinom(Nt, 1, d))
  pop <- (Nt + births - deaths) 
  return(pop)
}

```








```{r}

logisticPDynamics <- function(Nt, b, d, steps=1000, mod=logisticP1){
  ret <- c()
  ret[1] <- Nt
  if(length(b) == 1){
    b <- rep(b, steps)
  }
  if(length(d) == 1){
    d <- rep(d, steps)
  }
  for(i in 1:(steps-1)){
    ret[i+1] <- mod(ret[i], b=b[i], d=d[i])
  }
  return(ret)
}


plot(logisticPDynamics(10, b=0.1, d=0.1, mod=logisticP1, steps=10), type='l')

# birth death equal
plot(logisticPDynamics(10, b=0.1, d=0.1, mod=logisticP2, steps=10), type='l', ylim=c(0,30))
lapply(1:100, function(x){
  lines(logisticPDynamics(10, b=0.1, d=0.1, mod=logisticP2, steps=10))
})


# birth 2x death
plot(logisticPDynamics(10, b=0.2, d=0.1, mod=logisticP2, steps=10), type='l', ylim=c(0,50))
lapply(1:100, function(x){
  lines(logisticPDynamics(10, b=0.2, d=0.1, mod=logisticP2, steps=10))
})


# 1000 time steps
plot(logisticPDynamics(10, b=0.1, d=0.1, mod=logisticP2, steps=1000), type='l', ylim=c(0,200))
lapply(1:100, function(x){
  lines(logisticPDynamics(10, b=0.1, d=0.1, mod=logisticP2, steps=1000))
})


```






















But the above incorporation of probabilistic processes is based on phenomenological modeling, where many of us might also want to do some statistical modeling. Now we will go over probability distributions more generally in R, and how they relate to common statistical tests. 


## notation in R
“d” 	returns the height of the probability density function
“p” 	returns the cumulative density function
“q” 	returns the inverse cumulative density function (quantiles)
“r” 	returns randomly generated numbers



### Generate random draws from probability distributions

```{r}
#number, mean, sd
rnorm(100, 1, 1)
rbinom(100, 1, 0.25)
rpois(100, 1)
runif(100, 1, 20) #bounded between 1 and 20
rgamma(100, 1, 1) # 
rbeta(100, ) # two shape parameters bound between 0-1 


library(MASS)
fitdistr()
#use this to 
fitdistr(rpois(10,2), densfun = 'Poisson')

```



Given a number or a list it computes the probability that a random number will be less than that number. 

```{r}

pnorm(-1, mean=0, sd=1)
pnorm(1, mean=0, sd=1)
pnorm(1, mean=0, sd=1, lower.tail=FALSE)


pbinom(0, 1, 0.15)
ppois(1, lambda=2)
punif(0.25, 0, 1)


```


The next function we look at is `q----` which is the inverse of `p----`. The idea behind `q----` is that you give it a probability, and it returns the number whose cumulative distribution matches the probability. 

```{r}

pnorm(0.25, mean=0, sd=1)

qnorm(0.25, mean=0, sd=1)
pnorm(-0.6744898, mean=0, sd=1)


par(mfrow=c(1,2))
hist(rnorm(1000))
abline(v=0.25, lwd=2)
abline(v=-0.6744898, col='red', lwd=2)
plot(ecdf(rnorm(1000)))
abline(v=0.25, lwd=2)
abline(v=-0.6744898, col='red', lwd=2)

```




The last is `d----`, which we will go ahead and ignore for now, as the `r---`, `p----`, and `q----` variants tend to be a bit more useful for thinking about statistical testing. 

















### A case study of the t-test


The t-test can be used to test for differences between two groups of data, or of one group of data and some mean $\mu$. It is a comparison of means, so we implicitly assume no differences in variance or distributional shape between the two groups. 


### One sample
(X - mu) / (sd / sqrt(n))


```{r}
x <- rnorm(100, 1, 2)
mu <- 2

hist(x)
abline(v=mu, col='dodgerblue', lwd=2)

t.test(x, mu=mu)


tt <- (mean(x) - mu) / (sd(x) / sqrt(length(x)))
pt(tt, df=length(x)-1)


hist(rt(10000, df=99), xlim=c(-7,7)) 
abline(v=tt, lwd=3)


```








### Two-sample

X1-X2 / (sp)

sp = sqrt((varx1  + varx2) / 2)

sp is pooled variance

```{r}

x1 <- rnorm(100, 1, 2)
x2 <- rnorm(100, 2, 1)

t.test(x1, x2, var.equal=TRUE)


sp <- sqrt((var(x1)+var(x2)) / 2)
tt2 <- (mean(x1)-mean(x2)) / (sp*sqrt(2/length(x1)))

df <- (2*length(x1)) - 2

pt(tt2, df=df) * 2


hist(rt(10000, df=df), xlim=c(-5,5)) 
abline(v=tt2, lwd=3)


## at larger sample sizes, the normal distribution is approximately equal to the t-distribution (a z-test is the equivalent, which assumes that we know lots). The normal distribution is commonly used for significance testing (which we won't really go into here much beyond this t-test bit). 
pt(tt2, df=df)
pnorm(tt2)

```















