---
title: "Week 7"
author: "Tad Dallas"
includes:
  in_header:
    - \usepackage{lmodern}
output:
  pdf_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    toc: yes
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    highlight: tango
    theme: journal
---





## What's the point of APIs?

Application programming interfaces (or `API`s) are used to describe the communication of one computer to a web-based source of data in a programmatic manner. The computer (client) makes a request for data in a well-documented and consistent way, and receives data from the server. This is incredibly useful for acquiring data in a programmatic way, allowing analyses to be re-run on dynamic data, creating a living analysis. This could be critical in situations such as an ongoing pandemic, where daily case count updates might change model parameters or predictions. 

Many websites have APIs in order to allow users to make calls and develop apps on top of existing sites e.g., Facebook, Spotify, etc. all have APIs. 

Why is this valuable? Contrast the API approach to pure web scraping. Web scraping refers to extracting data from html pages. This is different from querying an API, as APIs will have documentation about how the data are represented. For instance, what happens when a website decides to change a bit of its formatting? This would potentially break any web scraping code, but the API backend would be maintained. 

In the context of biological data, many data repositories (figshare, data dryad, dataONE) have clear APIs. Further, it is becoming more common for large museum and government datasets to have developed APIs. I am a huge fan of this, as it is a departure from the longstanding "my data" mentality that has lead to a clear power dynamic (e.g., being associated with labs with lots of data gives them a clear advantage...but those data were likely gathered with a combination of taxpayer dollars and researcher sweat. The dollars trump the sweat, in my opinion.). 





## How do I query an API?

APIs can be queried programmtically using a number of `R` packages. Some APIs will promote their own `R` packages that have functions specific to their data structures or API. However, at the core of these packages are a small number of low-level packages for querying APIs. 

The general structure of an API call is the same relatively regardless of `R` package used. The parameters of an HTTP request are typically contained in the URL. For example, to return a map of Manchester using the Google Maps Static API we would submit the following request:

```{r, eval=FALSE}
https://maps.googleapis.com/maps/api/staticmap?center=Manchester,England&zoom=13&size=600x300&maptype=roadmap
```


The request contains:
+ a URL to the API endpoint (https://maps.googleapis.com/maps/api/staticmap?) and;
+ a query containing the parameters of the request (center=Manchester,England&zoom=13&size=600x300&maptype=roadmap). In this case, we have specified the location, zoom level, size and type of map.


Web service APIs use two key HTTP verbs to enable data requests: GET and POST. A GET request is submitted within the URL with each parameter separated by an ampersand (&). A POST request is submitted in the message body which is separate from the URL. The advantage of using POST over GET requests is that there are no character limits and the request is more secure because it is not stored in the browser’s cache.

There are several types of Web service APIs (e.g. XML-RPC, JSON-RPC and SOAP) but the most popular is Representational State Transfer or REST. RESTful APIs can return output as XML, JSON, CSV and several other data formats. Each API has documentation and specifications which determine how data can be transferred. 






 
```{r}

install.packages(c("httr", "jsonlite"))

```




After downloading the libraries, we will be able to use them in our `R` scripts or `RMarkdown` files.

```{r}

library(httr)
library(jsonlite)

```





## A simple example of an API call 

Some APIs require a token, in order to identify who is accessing the data (e.g., Spotify, Facebook, etc.). Developer access is pretty easy to get when you have an account, but it is a bit too much of a hassle for demonstration purposes. So we will use an open API from the Internet Archive's _Open Library_. 

We can send a request for data by using the documented API put out by the data provider, which consists of a base url (http://openlibrary.org/search.json?) with an amended query for searching the database. We use the GET verb, which is in `R` but stems from HTTP (hypertext transfer protocol). It stores all the information in an object that is structured in JSON. Below, we search the Open Library for "Vonnegut", and receive a nested list which includes information on status codes, among other relevant information. 


```{r}

von <- httr::GET('http://openlibrary.org/search.json?q=vonnegut')
str(von)

```


This data structure is basically in JSON format, which is a different file format (e.g., xml, html, etc.). To make it something that `R` can work with, we use the `jsonlite` package, and function `fromJSON`. 



```{r}

vonInfo <- jsonlite::fromJSON(content(von, "text"), simplifyVector = FALSE)

```


This output is a list of length 4, each containing a nested list. It is not the cleanest output, but it rarely is. But all the information you could want is buried somewhere here. 


This apply statement gets information on the title of each of the works returned from our search for "Vonnegut". 


```{r}

sapply(vonInfo[[4]], function(x){
  x$title
})

```


But Vonnegut as a search term is misleading, because many people have written books on Vonnegut, while we may want books _by_ Vonnegut. 







```{r}

von2 <- httr::GET('http://openlibrary.org/search.json?author=vonnegut')
str(von2)

```


This data structure is basically in JSON format, which is a different file format (e.g., xml, htm, etc.). To make it something that `R` can work with, we use the `jsonlite` package, and function `fromJSON`. 



```{r}

vonInfo2 <- jsonlite::fromJSON(content(von2, "text"), simplifyVector = FALSE)

```

This is better, as it now provides information on books where Vonnegut was listed as an author. 


```{r}

sapply(vonInfo2[[4]], function(x){
  x$title
})

```





> Side fun note: You can access Github info through API calls as well

```{r}

tad <- httr::GET('https://api.github.com/users/taddallas')
tadInfo <- jsonlite::fromJSON(content(tad, "text"), simplifyVector = FALSE)

```











## A more biological example


So this gives a good background about APIs, but it does not give a good biological example of when they are useful. The books that Vonnegut wrote are unlikely to change, so the search queries to this API are fairly static, and a bit removed from biology. 



One good biology API that is open is the Global Biodiversity Information Facility (GBIF). We can use a similar structure to our Open Library query to obtain information on species names and occurrences. 

```{r}

giraffe <- httr::GET('https://www.gbif.org/developer/species/Giraffa+camelopardalis')
str(giraffe)

```


Before we dive too far down this hole, it is important to note that this wheel has already been invented and is being maintained by the ROpenSci collective, a group of `R` programmers who develop tools for the access and analysis of data resources. We will use this package, not because it enhances reproducibility (as it is yet another dependency), but because it is well-written, well-documented, and from a group of scientists committed to helping make open data available and analyses reproducible. 


```{r}

install.packages('rgbif')
giraffe <- rgbif::occ_search(scientificName = "Giraffa camelopardalis", 
	limit = 5000)

```

> Depending on your operating system (OS) and previously installed packages, this is where we start to get into issues of dependencies. The `rgbif` package requires the installation of `rgeos`, which requires the `geos` library to be installed outside of `R`. Hopefully you will be able to successfully install the package, but it is an important note that handling these dependency and OS-specific issues is pretty central to making sure an analytical pipeline is reproducible. 


So presumably we have successfully queried the GBIF API using the `rgbif` package, and now have a data.frame of giraffe occurrences (limited to 5000 occurrence points). The output data are structured as a list of lists, which is a bit confusing if we look at the `str()` of `giraffe`, as the authors of the package have created a `class` called `gbif` to hold all the data. This is common in `R` packages, and the authors have written in some great functionality in printing and working with these data, formatting the data as a `tibble` object. We will not go too much into `tibble`s, but they are pretty useful for keeping data in a "tidy" format, as columns in a tibble can be lists (e.g., what if we wanted to store spatial polygon information, vectors, etc. alongside each row element? `tibble` handles this need effectively). 



```{r}

typeof(giraffe)
class(giraffe)

```



For simplicity sake, we can use some of the functionality within `rgbif` to just return the data we are interested in, specifying that we only want the raw data, not all the potential data (which includes images, etc.)


```{r}

install.packages('rgbif')
library(rgbif)

```

```{r}

giraffe2 <- rgbif::occ_search(scientificName = "Giraffa camelopardalis", 
	limit = 5000, return='data')[[3]]

```



We will quickly plot these data out just to show them in geographic space. We first create a base map using the `maps` package, which is a really easy way to plot geopolitical boundaries. Then we layer on the occurrence data using the `points` function.

```{r}

install.packages('maps')

maps::map()
points(giraffe2$decimalLongitude, giraffe2$decimalLatitude, pch=16, 
	col=grey(0.1,0.5))

```


Giraffes are in the US!!! These are almost certainly zoo records or errors. 















### Interfacing with local databases

Accessing data through APIs is fantastic, but there are plenty of times where you may need to access large data files locally. Note that this only is necessary if the data you are working with cannot fit into local memory. Otherwise, there is no real advantage to using a database framework, as it will likely be more expensive in terms of time and potential frustration. 


For this, we will have to include yet another dependency (5+ in this lecture alone), which of course has a bunch of other dependencies. This is not really ideal given the focus on reproducible research in the course, but it is sadly pretty much necessary. 


```{r}

install.packages('dbplyr')

```


When referring to "databases", we may think that there is a single type or schema, but this is definitely not the case. `R` has a way to accommodate for different database types, and that is to include a dependency for each database type (maybe not ideal). Below is a list describing the different ways `R` interfaces with different database types (taken from https://www.kaggle.com/anello/working-with-databases-in-r). 

+ RMySQL connects to MySQL and MariaDB
+ RPostgreSQL connects to Postgres and Redshift.
+ RSQLite embeds a SQLite database.
+ odbc connects to many commercial databases via the open database connectivity protocol.
+ bigrquery connects to Google’s BigQuery.


We will focus on the use of RSQLite as a backend to connect to SQLite databases. This will sadly require another dependency. 

```{r}

install.packages('RSQLite')

```






```{r}

dir.create("data_raw", showWarnings = FALSE)
download.file(url = "https://ndownloader.figshare.com/files/2292171",
              destfile = "data_raw/portal_mammals.sqlite", mode = "wb")
```




```{r}
install.packages('DBI')
library(DBI)

mammals <- DBI::dbConnect(RSQLite::SQLite(), "data_raw/portal_mammals.sqlite")
dbplyr::src_dbi(mammals)

```


Just like a spreadsheet with multiple worksheets, a SQLite database can contain multiple tables. In this case three of them are listed in the tbls row in the output above:

+ plots
+ species
+ surveys


Now that we know we can connect to the database, let's explore how to get the data from its tables into R.

### Querying the database with the SQL syntax

To connect to tables within a database, you can use the `tbl()` function from `dplyr`. This function can be used to send SQL queries to the database. To demonstrate this functionality, let’s select the columns "year", "species_id", and "plot_id" from the surveys table:


```{r}

dplyr::tbl(mammals, sql("SELECT year, species_id, plot_id FROM surveys"))

```



With this approach you can use any of the SQL queries we have seen in the database lesson.


### Querying the database with the dplyr syntax

One of the strengths of dplyr is that the same operation can be done using dplyr verbs instead of writing SQL. First, we select the table on which to do the operations by creating the surveys object, and then we use the standard dplyr syntax as if it were a data frame:

```{r}
surveys <- dplyr::tbl(mammals, "surveys")
surveys %>%
    dplyr::select(year, species_id, plot_id)
```

In this case, the surveys object behaves like a data frame. Several functions that can be used with data frames can also be used on tables from a database. For instance, the `head()` function can be used to check the first 10 rows of the table:


```{r}

head(surveys, n = 10)

```





```{r}

surveys2 <- surveys %>%
  dplyr::filter(weight < 5) %>%
  dplyr::select(species_id, sex, weight)

```




`R` makes lazy calls to databases, until you make it not be lazy. That is, everything is held outside of memory until you tell `R` that some portion of the data should not be. 

```{r}

head(surveys2)

```



To pull the data into memory and allow us to use the data, we must use the `collect` function. 

```{r}

surveys3 <- dplyr::collect(surveys2)
head(surveys3)

```



This difference becomes clear when we try to index a specific column of data and work with it. 

```{r}

surveys2$sex
surveys3$sex

```











### Relating this back to the `dplyr` syntax of joins

We discussed joins in the data manipulation section. Joins are pretty key to working with databases, as much of the benefit of database structure is from nested data and the utility of key values. For instance, in our mammal sampling data, we have plot level data...

```{r}

plots <- dplyr::tbl(mammals, "plots")
plots

```


and survey-level data, where plot-level data provides what is essentially metadata for each survey. 


We can use joins to combine and manipulate these data, as we did before, but now in the context of the database structure.



```{r}

survPlot <- dplyr::left_join(surveys, plots, by='plot_id')

```





















































# Spatial data


Spatial phenomena can generally be thought of as either discrete objects with clear boundaries or as a continuous phenomenon that can be observed everywhere, but does not have natural boundaries. Discrete spatial objects may refer to a river, road, country, town, or a research site. Examples of continuous phenomena, or “spatial fields”, include elevation, temperature, and air quality.


Spatial objects are usually represented by vector data. Such data consists of a description of the _geometry_ or _shape_ of the objects, and normally also includes additional variables. For example, vector data can describe geographic border data while also containing information on attributes of bounded geographic locations (e.g., vector data describing borders of a country could include attribute data at smaller municipal locations like counties). In this lecture, we will take a fairly shallow dive into spatial data representation and manipulation in `R`. 


This lecture would not have been possible without the help of Robert Hijmans and his work on Spatial Data Science (https://rspatial.org/raster/spatial/). For a much deeper dive into working with spatial data in R, please see that resource. 







## Vector data

The main vector data types are points, lines and polygons. In all cases, the geometry of these data structures consists of sets of coordinate pairs (x, y). Points are the simplest case. Each point has one coordinate pair, and $n$ associated variables. 


The geometry of lines is a just a little bit more complex. Lines are represented as ordered sets of coordinates (nodes). The actual line segments can be computed (and drawn on a map) by connecting the points. Thus, the representation of a line is very similar to that of a multi-point structure. The main difference is that the ordering of the points is important, because we need to know which points should be connected. A network (e.g. a road or river network), or spatial graph, is a special type of lines geometry where there is additional information about things like flow, connectivity, direction, and distance.


A polygon refers to a set of closed lines. The geometry is very similar to that of lines, but to close a polygon the last coordinate pair coincides with the first pair. A complication with polygons is that they can have holes (that is a polygon entirely enclosed by another polygon, that serves to remove parts of the enclosing polygon). For instance, consider a polygon of land mass which has multiple lakes within the polygon which describes the land mass. Valid polygons do not self-intersect, but multiple polygons can be considered as a single geometry. For example, Indonesia consists of many islands. Each island can be represented by a single polygon, but together then can be represent a single (multi-) polygon representing the entire country. 


















## Raster data

Raster data is commonly used to represent spatially continuous phenomena such as elevation. A raster divides the world into a grid of equally sized rectangles (referred to as cells or, in the context of satellite remote sensing, pixels) that all have one or more values (or missing values) for the variables of interest. A raster cell value should normally represent the average (or majority) value for the area it covers. However, in some cases the values are actually estimates for the center of the cell (in essence becoming a regular set of points with an attribute).

In contrast to vector data, in raster data the geometry is not explicitly stored as coordinates. It is implicitly set by knowing the spatial extent and the number or rows and columns in which the area is divided. From the extent and number of rows and columns, the size of the raster cells (spatial resolution) can be computed. Think of rasters as a matrix of spatial data, where each cell of the matrix corresponds to a block of spatial data and some variable(s) measured at each matrix cell. 




































## Simple representation of spatial data

The basic data types in R are numbers, characters, logical (TRUE or FALSE) and factor values. Values of a single type can be combined in vectors and matrices, and variables of multiple types can be combined into a data.frame. We can represent (only very) basic spatial data with these data types. Let’s say we have the location (represented by longitude and latitude) of ten weather stations (named A to J) and their annual precipitation.

In the example below we make a very simple map. Note that a map is special type of plot (like a scatter plot, barplot, etc.). A map is a plot of geospatial data that also has labels and other graphical objects such as a scale bar or legend. The spatial data itself should not be referred to as a map.


### Libraries
We'll use three out of the many many many spatial libraries that exist. We'll use `sp`, `raster`, and `rgdal`. You may run into installation issues, since at least `rgdal` relies on a backend library already existing. This is already a signal that reproducibility with spatial data is an inherent challenge. The last lecture, which went over docker and setting up a computational environment explicitly in a container, might seem a bit more worthwhile after hitting issues with spatial data packages. 


```{r}
install.packages(c('sp', 'raster', 'rgdal'))
```



```{r}

name <- LETTERS[1:10]
longitude <- c(-116.7, -120.4, -116.7, -113.5, -115.5,
               -120.8, -119.5, -113.7, -113.7, -110.7)
latitude <- c(45.3, 42.6, 38.9, 42.1, 35.7, 38.9,
              36.2, 39, 41.6, 36.9)


# Simulated rainfall data
set.seed(0)
precip <- round((runif(length(latitude))*10))


stations <- data.frame(longitude, latitude, name, precip)

```




This is vector data.


```{r}

par(mar=c(4,4,0.5,0.5))

# adding points
plot(stations[,1:2], 
	cex=stations$precip, 
	pch=16, 
	col='dodgerblue')
text(stations, name, pos=4)

# adding polygons 
polygon(stations[1:3, 1:2], 
	col=adjustcolor('firebrick', 0.25))

# adding lines
lines(stations[3:6, 1:2], lwd=2, col=grey(0.5,0.5))

```



Polygons need to _closed_, that is, the first point must coincide with the last point, but the polygon function took care of that for us.













## sp data

Package sp is the central package supporting spatial data analysis in R. sp defines a set of classes to represent spatial data. A class defines a particular data type. The data.frame is an example of a class. Any particular data.frame you create is an object (instantiation) of that class.

The main reason for defining classes is to create a standard representation of a particular data type to make it easier to write functions (also known as ‘methods’) for them. In fact, the sp package does not provide many functions to modify or analyze spatial data; but the classes it defines are used in more than 100 other R packages that provide specific functionality. See Hadley Wickham’s Advanced R or John Chambers’ Software for data analysis for a detailed discussion of the use of classes in R).

We will be using the sp package here. Note that this package will eventually be replaced by the newer sf package — but sp is still more commonly used.

Package sp introduces a number of classes with names that start with Spatial. For vector data, the basic types are the SpatialPoints, SpatialLines, and SpatialPolygons. These classes only represent geometries. To also store attributes, classes are available with these names plus DataFrame, for example, SpatialPolygonsDataFrame and SpatialPointsDataFrame. When referring to any object with a name that starts with Spatial, it is common to write Spatial*



```{r}

#install.packages('sp')
library(sp)

pts <- sp::SpatialPoints(stations[,1:2])


typeof(pts)
class(pts)
str(pts)

```











### Coordinate reference systems. 

There is also a “proj4string”. This stores the coordinate reference system (“crs”, discussed in more detail later). We did not provide the crs so it is unknown (NA). That is not good, so let’s recreate the object, and now provide a crs.


```{r}

pts <- sp::SpatialPoints(pts, proj4string=CRS("+proj=longlat +datum=WGS84"))
ptsdf <- sp::SpatialPointsDataFrame(pts, data=stations[,3:4])

```







### Manipulating vector data


```{r}

str(ptsdf)
head(ptsdf@data)
head(ptsdf)

```







Built-in functionality to work with base R syntax is present for most of these types of data structures. For instance, the addition of a vector of data can be done in the same manner as you would add a variable to a data.frame. 


```{r}

ptsdf$random <- runif(nrow(ptsdf))
ptsdf

```





This SpatialPointsDataFrame can be subset like any data.frame or matrix 

```{r}

ptsdf2 <- ptsdf[which(ptsdf$random < 0.5), ]
ptsdf2

```




Then we can visualize the points that are included in both data.frames (the whole `ptsdf` and the subset `ptsdf` where our random variable is less than 0.5). 


```{r}

plot(ptsdf, col='red', cex=2)
points(ptsdf2, pch=16, col='dodgerblue')

```













## Raster data

The `raster` package has functions for creating, reading, manipulating, and writing raster data. The package provides, among other things, general raster data manipulation functions that can easily be used to develop more specific functions. For example, there are functions to read a chunk of raster values from a file or to convert cell numbers to coordinates and back. The package also implements raster algebra and many other functions for raster data manipulation.





First, we will install the `raster` package and create our first simple raster. We give it an "extent" which mirrors latitude and longitude coordinates. An extent is just the bounding box of values which the raster, as a whole, covers. This is important to make the dimensions of the raster mean something, as otherwise we would simply assume that each raster cell is $n$ units wide by $m$ units tall. 

```{r}

#install.packages('raster')
library(raster)

rast <- raster(ncol=100, nrow=100, 
	xmx=-80, xmn=-150, ymn=20, ymx=60)

values(rast) <- runif(ncell(rast))

rast
str(rast)

```




By delineating the extent of the raster, we can make this gridded data comparable with point, line, and polygon data. For instance, the station data we created earlier based on latitude and longitude can be plotted onto this raster object. 


```{r}

plot(rast)
# adding polygons 
polygon(stations[1:3, 1:2], 
	col=adjustcolor('firebrick', 0.25))

# adding lines
lines(stations[3:6, 1:2], lwd=2, col=1)

```







Raster data, as plotted above, correspond to a single $z$ value or covariate (e.g., mean annual temperature), but often we have information on multiple climatic layers. We can keep these layers together by forming what is claled a "stack". 

```{r}

rastStack <- stack(rast, 
	rast*runif(100*100), 
	rast*runif(100*100),
	rast*10)

plot(rastStack)

```


This is useful because raster operations can now be done across all the rasters at the same time, much in the way forming objects into a list allows the use of `lapply` and similar functions.
















### Manipulating raster data

Raster data, as noted above, are in the form of a 2-dimensional array (e.g., a matrix). But we say when we examined the `str()` of a raster object that there was a bit more nuance to this. The developers of `raster` have made it so that the functionality of interacting with matrices in R (at least in terms of indexing) also corresponds to indexing raster objects. 

```{r}

rast[1:4]

rast[1:3,1:3]

# keep the matrix structure
rast[1:3,1:3, drop=FALSE]

```



















#### Raster-specific commands for data manipulation:

A property representing the bounds of a raster object is the `extent` (mentioned briefly above). This property of the raster can be modified to focus on a particular area of the raster or to simply view the inherited extent of your raster object. 


```{r}

extent(rast)

extent(rast, 1,10,1,10)

```




This can be useful if we want to focus on a specific region of the raster, and specifically if we know which rectangular extent we wish to focus on. The cropping of a raster object is made possible with the `crop` function, which takes the input raster object `x` and an extent function `y`. This means that the second argument (the `y`) must be in the form of an extent object (created usin gthe `extent()` function described above). 



```{r}
crast <- rast
raster::extent(crast)

crast <- raster::crop(crast, extent(-150,-100,20,60))

```









But while this is incredibly useful for subsetting data down to a specific region or bounding box, we may want to consider merging raster files of different extents. First, it's important to make sure that the units match. This is not _as_ big of a deal for raster data as it is for _true_ spatial data which has a clear coordinate reference system, as this controls how spatial data are treated and visualized. Recall that raster data are gridded data, so comparing raster data simply requires that they have comparable units for their extents. 

```{r}

r1 <- raster::crop(rast, extent(-140,-120, 0, 30))
r2 <- raster::crop(rast, extent(-100, -80, 30, 40))
m <- raster::merge(r1, r2)
plot(m)

```






A common situation related to spatial data in R is one in which the researcher wants to extract a set of values from a continuous surface of some covariate, where the continuous surface is most often represented as a raster and the set of values is typical a set of coordinates. There are a few different ways to do this, with varying levels of complexity. 


Rasters can be subset by their row and column indices: 

```{r}

cells <- raster::cellFromRowCol(rast, 50, 35:39)
cells

rast[cells]

```





Or, perhaps more simply, using the `extract` function within the `raster` package. Here, `xyFromCells` provides raster coordinates that correspond to certain cell ids (set above using the `cellFromRowCol` function). Here, `extract` will pull out the values in a raster corresponding to a given set of x and y values. To frame this in a context, this could correspond to extracting values for a specific set of latitude and longitude coordinates from a raster of mean annual temperature. 


```{r}

raster::extract(rast, cells)

xy <- raster::xyFromCell(rast, cells)
xy

raster::extract(rast, xy)

```







Raster objects may be converted into `SpatialPointsDataFrame` or `SpatialPolygonsDataFrame` objects, which is incredibly useful, but also may introduce some odd resolution issues, as it enforces the resolution of the raster (a lattice) into a continuous data structure. 



```{r}

r1sp <- as(r1, 'SpatialPolygonsDataFrame')

plot(r1)

```


Related to this point, in terms of efforts to bridge point/polygon data to raster data, are two sets of functions. First, `rasterize` overlays a lattice on point, line, or polygon data to enforce a raster structure onto more "continuous" spatial data. 

```{r}

rpts <- rasterize(pts, rast)


plot(rpts, xlim=c(-130,-105), ylim=c(30,50))
points(pts)

``` 



Notice the slight jitter in the above plot. This is because forcing the raster structure on points data influences the relative position of points according to the resolution of the raster (a finer resolution would produce more agreement between point data and rasterized output). 



We can also consider outputting the raster data to xy point data or polygon data, which we would use `rasterToPoints` or `rasterToPolygons` functions.


```{r}

pts2 <- raster::rasterToPoints(rpts)

plot(rpts, xlim=c(-130,-105), ylim=c(30,50))
points(pts)
points(pts2, pch=16, col='red')

```

There are also ways to examine the shared overlap between raster objects. However, these methods require the user to have R packages `rgdal` and `rgeos`, both of which require backend libraries `gdal` and `geos`, respectively. It doesn't take long to see that spatial data analyses may suffer from some of the reproducibility concerns that we've gone into thus far. 










## Working with external data 


Instead of creating point, line, polygon, or raster data, let us now look at times where we pull data in from external sources. To do this, we will need the package `rgdal`, which may cause some errors when downloading, but this is just how using `R` as a GIS system goes sometimes. 


```{r}

#install.packages('rgdal')
library(rgdal)

#reading data
f <- raster(system.file("external/rlogo.grd", package="raster"))

plot(f, col=rainbow(1000))

#writing data
raster::writeRaster(f, 'output.tif', overwrite=TRUE)

```




A less trivial example would be working with large-scale climatic data, something which many of us will find ourselves doing at some point. Here, I downloaded a single layer of the WorldClim data corresponding to average global temperature at 10 minute resolution (a coarse resolution corresponding to around 18km cell length and width at the equator). 


```{r}

tavg1 <- raster('tavg/wc2.1_10m_tavg_01.tif')
str(tavg1)

plot(tavg1)

```








Using a raster stack in a realistic setting.

```{r}

tavg <- raster::stack(
	raster('tavg/wc2.1_10m_tavg_01.tif'),
	raster('tavg/wc2.1_10m_tavg_02.tif'),
	raster('tavg/wc2.1_10m_tavg_03.tif'),
	raster('tavg/wc2.1_10m_tavg_04.tif'),
	raster('tavg/wc2.1_10m_tavg_05.tif'),
	raster('tavg/wc2.1_10m_tavg_06.tif'),
	raster('tavg/wc2.1_10m_tavg_07.tif'),	
	raster('tavg/wc2.1_10m_tavg_08.tif'),
	raster('tavg/wc2.1_10m_tavg_09.tif'),
	raster('tavg/wc2.1_10m_tavg_10.tif'),
	raster('tavg/wc2.1_10m_tavg_11.tif'),
	raster('tavg/wc2.1_10m_tavg_12.tif')
)	
	
```








For instance, we can extract specific values from the raster stack of mean temperatures. In this case, we use the `extract` function from the `raster` package to extract temperature data for Baton Rouge, Louisiana. It is hot here.


```{r}

brTemp <- raster::extract(tavg, data.frame(lon=-91.1, lat=30.47))

plot(1:12, brTemp, 
	xlab='Month', 
	ylab='Average temperature (C)', 
	pch=16, type='b')
	
```






















